{"cells":[{"cell_type":"markdown","metadata":{},"source":["### This is the same [`bigram-gpt.py`](./bigram-gpt.py) file in one cell. It was trained on Google COLAB of a GPU."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":488357,"status":"ok","timestamp":1706735960417,"user":{"displayName":"Harsh Vardhan Rana","userId":"00416889182280809693"},"user_tz":-330},"id":"DXcy9r-CWFEK","outputId":"9c2c4b04-ead5-4560-8648-5d95cd55df5b"},"outputs":[{"name":"stdout","output_type":"stream","text":["step 0: train loss 4.1789, val loss 4.1793\n","step 500: train loss 1.8545, val loss 1.9896\n","step 1000: train loss 1.6054, val loss 1.7864\n","step 1500: train loss 1.5319, val loss 1.7280\n","step 2000: train loss 1.4944, val loss 1.6953\n","step 2500: train loss 1.4737, val loss 1.6914\n","step 3000: train loss 1.4501, val loss 1.6704\n","step 3500: train loss 1.4376, val loss 1.6610\n","step 4000: train loss 1.4295, val loss 1.6516\n","step 4500: train loss 1.4205, val loss 1.6576\n","\n","\n","man At which gotchforty thou tell, people thy.\n","Come, my heaven pursent thou genck in done.\n","Alas! Who, thou and st, my lords give\n","citizens\n","Third Conty lift but of yours; blood formies:\n","queany you me;\n","We'll we she drive hither.\n","\n","LUCIO:\n","Having o' the 'tis the deep thing to come all an shore to contenter woe.\n","\n","First Watchman:\n","From his with the gave him.\n","\n","COMINIUS:\n","And here thazard crown'd breathlod,\n","Imost\n","dittuzedy!\n","Most Engla, and to combs, is then shorty sharns\n","soli.\n","\n","CORIOLANUS:\n","The deeven lies,\n"]}],"source":["\"\"\"\n","Script to run a bigram model for character level text generation.\n","\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# hyperparameters\n","batch_size = 64\n","block_size = 256\n","max_iters = 5000\n","eval_internals = 500\n","learning_rate = 3e-4\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","eval_iters = 200\n","n_embd = 384\n","n_head = 6\n","n_layer = 6\n","dropout = 0.2\n","\n","torch.manual_seed(1337)\n","\n","# read in the data\n","with open('/content/drive/MyDrive/Colab Notebooks/gpt-from-scratch/data/input.txt', 'r', encoding='UTF-8',) as f:\n","    text = f.read()\n","\n","# list the unique characters that occur in text\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","\n","# map the characters with numbers\n","stoi = { ch:i for i, ch in enumerate(chars) }\n","itos = { i:ch for i, ch in enumerate(chars) }\n","encode = lambda s: [stoi[c] for c in s]\n","decode = lambda l: ''.join([itos[i] for i in l])\n","\n","# encode the entire dataset and store it in a tensor\n","data = torch.tensor(encode(text), dtype=torch.long)\n","\n","# seperate dataset into train and validation set\n","n = int(0.9 * len(data))\n","train_data = data[:n]\n","val_data = data[n:]\n","\n","# data loading\n","def get_batch(split):\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x, y\n","\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","\n","class Head(nn.Module):\n","    \"\"\"one head of self-attention\"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        B, T, C = x.shape\n","        k = self.key(x)\n","        q = self.query(x)\n","\n","        # compute attention scores\n","        wei = q @ k.transpose(-2, -1) * C ** -0.5\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n","        wei = F.softmax(wei, dim=-1)\n","        wei = self.dropout(wei)\n","\n","        # weighted aggregation of values\n","        v = self.value(x)\n","        out = wei @ v\n","        return out\n","\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\"multiple heads of self attention in parallel\"\"\"\n","\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(n_embd, n_embd)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.proj(out)\n","        return out\n","\n","\n","class FeedForward(nn.Module):\n","\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd),\n","            nn.ReLU(),\n","            nn.Linear(4 * n_embd, n_embd),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","\n","class Block(nn.Module):\n","    \"\"\"Transformer block: communication followed by computation\"\"\"\n","\n","    def __init__(self, n_embd, n_head):\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.sa = MultiHeadAttention(n_head, head_size)\n","        self.ffwd = FeedForward(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x))\n","        x = x + self.ffwd(self.ln2(x))\n","        return x\n","\n","# simple bigram model\n","class BigramLanguageModel(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n","        # self.blocks = nn.Sequential(\n","        #     Block(n_embd, n_head=4),\n","        #     Block(n_embd, n_head=4),\n","        #     Block(n_embd, n_head=4),\n","        #     nn.LayerNorm(n_embd),\n","        # )\n","        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n","        self.sa_heads = MultiHeadAttention(4, n_embd//4)\n","        self.ffwd = FeedForward(n_embd)\n","        self.lm_head = nn.Linear(n_embd, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape\n","\n","        tok_emb = self.token_embedding_table(idx)  # (b, t, c)\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n","        x = tok_emb + pos_emb\n","        x = self.sa_heads(x)\n","        x = self.ffwd(x)\n","        logits = self.lm_head(x)  # (b, t, vocab_size)\n","\n","        if targets==None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        for _ in range(max_new_tokens):\n","            idx_cond = idx[:, -block_size:]\n","            logits, loss = self(idx_cond)\n","            logits = logits[:, -1, :]\n","            probs = F.softmax(logits, dim=1)\n","            idx_next = torch.multinomial(probs, num_samples=1)\n","            idx = torch.cat((idx, idx_next), dim=1)\n","        return idx\n","\n","model = BigramLanguageModel()\n","m = model.to(device)\n","\n","# create PyTorch optimizer\n","optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n","\n","for iter in range(max_iters):\n","\n","    if iter % eval_internals == 0:\n","        losses = estimate_loss()\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    # sample a batch of data\n","    xb, yb = get_batch('train')\n","\n","    # evaluate the loss\n","    logits, loss = m(xb, yb)\n","    optimizer.zero_grad(set_to_none=None)\n","    loss.backward()\n","    optimizer.step()\n","\n","# generate from the model\n","context = torch.zeros((1, 1), dtype=torch.long, device=device)\n","print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DB_5UfjUXImz"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN9UU3CTmZvOkfIlPs25C0d","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
